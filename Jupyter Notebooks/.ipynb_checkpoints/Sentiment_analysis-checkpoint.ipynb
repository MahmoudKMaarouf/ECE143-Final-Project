{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle5 as pickle\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackOverflowParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.strict = False  # If any invalid html is encountered, parser will make a best guess at its intention\n",
    "        self.convert_charrefs = True  # Hold data section until next tag is encountered\n",
    "\n",
    "        # Field variable to keep track of parsed data with tags removed\n",
    "        self.text = StringIO()\n",
    "        self.text_no_code = StringIO()\n",
    "\n",
    "        # Field variables to keep track of and store <code></code> blocks\n",
    "        self.code_blocks = []\n",
    "        self.lasttag = None\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        '''\n",
    "        Method inherited from HTMLParser super class that is called whenever the start of a tag is encountered.\n",
    "        In this parser, it keeps track of the last start tag that was encountered.\n",
    "        :param tag: Current tag being parsed (ex: p, div, code, etc.)\n",
    "        :type tag: str\n",
    "        :param attrs: List of (name,value) pairs containing attributes found inside the tag's brackets\n",
    "        :type attrs: list[str]\n",
    "        '''\n",
    "        assert isinstance(tag, str)\n",
    "        assert isinstance(attrs, list)\n",
    "\n",
    "        self.lasttag = tag\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        '''\n",
    "        Method inherited from HTMLParser super class that is called whenever data inside of a tag is encountered.\n",
    "        In this parser, it saves blocks of code to the field variable self.code and records all text with HTML tags removed\n",
    "        :param data: Current data inside of a tag being parsed\n",
    "        :type tag: str\n",
    "        '''\n",
    "        assert isinstance(data, str)\n",
    "\n",
    "        # If the last tag encountered was a <code> tag, append the contents to the list of code blocks\n",
    "        if self.lasttag == \"code\":\n",
    "            self.lasttag = None\n",
    "            self.code_blocks.append(data)\n",
    "        else:\n",
    "            self.text_no_code.write(data)\n",
    "\n",
    "        # Record text between tags\n",
    "        self.text.write(data)\n",
    "\n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Returns parsed text without HTML tags\n",
    "        :return: Text wihtout tags\n",
    "        :type return: str\n",
    "        '''\n",
    "        return self.text.getvalue()\n",
    "\n",
    "    def get_data_no_code(self):\n",
    "        '''\n",
    "        Returns parsed text without HTML tags and with code blocks removed\n",
    "        :return: Text wihtout tags\n",
    "        :type return: str\n",
    "        '''\n",
    "        return self.text_no_code.getvalue()\n",
    "\n",
    "\n",
    "def strip_tags(html):\n",
    "    '''\n",
    "    Takes in a body of text that is formatted in HTML and returns the same text with the HTML tags now removed.\n",
    "    This method bundles the process of instantiating a parser, feeding the data, and returning the parsed output.\n",
    "    :param html: HTML-formatted body of text\n",
    "    :type html: str\n",
    "    :return: The input text now without HTML tags\n",
    "    :type return: str\n",
    "    '''\n",
    "    assert isinstance(html, str)\n",
    "\n",
    "    # Feed text into parser and return parsed text without tags\n",
    "    s = StackOverflowParser()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def get_text_no_code(html):\n",
    "    '''\n",
    "    Takes in a body of text that is formatted in HTML and returns the same text with the HTML tags and blocks of code now removed.\n",
    "    This method bundles the process of instantiating a parser, feeding the data, and returning the parsed output.\n",
    "    :param html: HTML-formatted body of text\n",
    "    :type html: str\n",
    "    :return: The input text now without HTML tags or code blocks\n",
    "    :type return: str\n",
    "    '''\n",
    "    assert isinstance(html, str)\n",
    "\n",
    "    # Feed text into parser and return parsed text without tags\n",
    "    s = StackOverflowParser()\n",
    "    s.feed(html)\n",
    "    return s.get_data_no_code()\n",
    "\n",
    "\n",
    "def get_code(html):\n",
    "    '''\n",
    "    Takes in a body of text that is formatted in HTML and returns the blocks of code found within the text.\n",
    "    This method bundles the process of instantiating a parser, feeding the data, and returning the blocks of code.\n",
    "    An empty list is returned if no <code> tags are found.\n",
    "    :param html: HTML-formatted body of text\n",
    "    :type html: str\n",
    "    :return: List of blocks of code found within text\n",
    "    :type return: list[str]\n",
    "    '''\n",
    "    assert isinstance(html, str)\n",
    "\n",
    "    s = StackOverflowParser()\n",
    "    s.feed(html)\n",
    "    return [item.replace('\\n', ' ') for item in s.code_blocks]\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file_questions = 'Questions.csv'\n",
    "file_answers = 'Answers.csv'\n",
    "file_tags = 'Tags.csv'\n",
    "\n",
    "dates = [\"CreationDate\"]\n",
    "\n",
    "\n",
    "\n",
    "# # Load dataframes (only loading first 10000 rows for now to reduce processing time)\n",
    "# questions_df = pd.read_csv(file_questions, nrows=1000, encoding='iso-8859-1')\n",
    "# answers_df = pd.read_csv(file_answers, nrows=1000, encoding='iso-8859-1')\n",
    "questions_df = pd.read_csv(file_questions, encoding='iso-8859-1', nrows=10000)\n",
    "answers_df = pd.read_csv(file_answers, encoding='iso-8859-1', nrows=10000)\n",
    "# tags_df = pd.read_csv(file_tags, encoding = 'iso-8859-1', nrows=10000)\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Add extra columns to dataframes\n",
    "# This takes a long time (~10 minutes) to process the entire dataset\n",
    "# Might be worth exploring the pandas to_pickle() method for saving/loading the dataframes\n",
    "# questions_df['Body_no_tags'] = questions_df['Body'].apply(strip_tags)\n",
    "questions_df['Body_no_tags_no_code'] = questions_df['Body'].apply(get_text_no_code)\n",
    "# questions_df['Body_code'] = questions_df['Body'].apply(get_code)\n",
    "\n",
    "# answers_df['Body_no_tags'] = answers_df['Body'].apply(strip_tags)\n",
    "answers_df['Body_no_tags_no_code'] = answers_df['Body'].apply(get_text_no_code)\n",
    "# answers_df['Body_code'] = answers_df['Body'].apply(get_code)\n",
    "\n",
    "\n",
    "# Row 11 in Questions.csv is a good example with a few code blocks\n",
    "# body = questions_df['Body'][11]\n",
    "# body_no_tags = questions_df['Body_no_tags'][11]\n",
    "# body_code = questions_df['Body_code'][11]\n",
    "# body_no_tags_no_code = questions_df['Body_no_tags_no_code'][11]\n",
    "questions = questions_df['Body_no_tags_no_code']\n",
    "answers = answers_df['Body_no_tags_no_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(comment):\n",
    "    '''\n",
    "    This function calculate the polarities of each strings from the preprocessed questions/answers strings and store\n",
    "    them into a dictionary.\n",
    "    :param comment:\n",
    "    :return:\n",
    "    '''\n",
    "    dict_sentiment = {}\n",
    "    for feedback in range(len(comment)):\n",
    "        pol = TextBlob(comment[feedback]).sentiment.polarity\n",
    "        dict_sentiment[feedback] = pol\n",
    "\n",
    "    return dict_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_dict = sentiment_analysis(questions)\n",
    "answer_dict = sentiment_analysis(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_polarities(adict):\n",
    "    pos_dict = {}\n",
    "    neu_dict = {}\n",
    "    neg_dict = {}\n",
    "    for key, values in adict.items():\n",
    "        if values > 0:\n",
    "            pos_dict[key] = values\n",
    "        elif values == 0:\n",
    "            neu_dict[key] = values\n",
    "        else:\n",
    "            neg_dict[key] = values\n",
    "\n",
    "    return pos_dict, neu_dict, neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pos, q_neu, q_neg = find_polarities(question_dict)\n",
    "a_pos, a_neu, a_neg = find_polarities(answer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_general_tone(adict):\n",
    "    all_val = adict.values()\n",
    "    max_val = max(all_val)\n",
    "    min_val = min(all_val)\n",
    "\n",
    "    avg = sum(adict.values()) / len(adict)\n",
    "\n",
    "    return max_val, min_val, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_general_tone(question_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_overall():\n",
    "    q_max, q_min, q_avg = find_general_tone(question_dict)\n",
    "    a_max, a_min, a_avg = find_general_tone(answer_dict)\n",
    "    w = 0.2\n",
    "    bar1 = np.arange(1)\n",
    "    bar2 = bar1 + w\n",
    "    bar3 = bar2 + w\n",
    "    font = {'fontname': 'Nunito'}\n",
    "    plt.subplot(1,2,1)\n",
    "\n",
    "    # plt.bar(bar, [q_max, q_min, q_avg], w, label='Most Positive Polarity')\n",
    "    plt.bar(bar1, q_max, w, color=(152/255, 223/255, 138/255), label='Most Positive Polarity')\n",
    "    plt.bar(bar2, q_min, w, color=(255/255, 152/255, 150/255), label='Most Negative Polarity')\n",
    "    plt.bar(bar3, q_avg, w, color=(158/255, 218/255, 229/255), label='Average Polarity')\n",
    "\n",
    "    # plt.rcParams['font.family'] = 'Nunito'\n",
    "    plt.title('Questions', fontsize=16, **font)\n",
    "    plt.ylabel('Polarity', fontsize=16, **font)\n",
    "    plt.xticks([], [])\n",
    "    plt.legend(prop={'family': 'Nunito', 'size':15})\n",
    "    plt.rcParams.update({'font.family': 'Nunito'})\n",
    "    # plt.show()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.bar(bar1, a_max, w, color=(152/255, 223/255, 138/255), label='Most Positive Polarity')\n",
    "    plt.bar(bar2, a_min, w, color=(255/255, 152/255, 150/255), label='Most Negative Polarity')\n",
    "    plt.bar(bar3, a_avg, w, color=(158/255, 218/255, 229/255), label='Average Polarity')\n",
    "    plt.xticks([], [])\n",
    "\n",
    "    plt.ylabel('Polarity', fontsize=16, **font)\n",
    "    plt.title('Answers', fontsize=16, **font)\n",
    "    # plt.bar(bar1, max_values, w, label='Most Positive Polarity')\n",
    "    # plt.bar(bar2, min_values, w, label='Most Negative Polarity')\n",
    "    # plt.bar(bar3, avg_values, w, label='Average Polarity')\n",
    "    # plt.xticks(bar1+w, 'Questions')\n",
    "\n",
    "    plt.legend(prop={'family': 'Nunito', 'size':15})\n",
    "    plt.rcParams.update({'font.family': 'Nunito'})\n",
    "    #\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_overall()\n",
    "\n",
    "\n",
    "ldamulticore = pickle.load(open(\"ldamulticore_100_QA.pkl\", \"rb\"))\n",
    "# pprint(ldamulticore.show_topics(num_words=5,formatted=False))\n",
    "topics = [[(term, round(wt, 3)) for term, wt in ldamulticore.show_topic(n, topn=20)] for n in range(0, ldamulticore.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic]) for topic in topics], columns=['Terms per Topic'], index=['Topic'+str(t) for t in range(1, ldamulticore.num_topics+1)])\n",
    "\n",
    "print(topics_df)\n",
    "topics_split = topics_df['Terms per Topic'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsentiment_topics(x):\n",
    "    '''\n",
    "    This function checks if any word in each topic word list also exists in the strings of the questions/answers.\n",
    "    If so, calculate the polarity of that string and get the average polarity for all the topics.\n",
    "    The function should return a dictionary where keys are Topic1-100 and values are the average polarities of the\n",
    "    corresponding topics.\n",
    "    '''\n",
    "    topic_dict = {}\n",
    "    for i in range(len(topics_split)):\n",
    "        count = 0\n",
    "        pol = 0\n",
    "        word_list = [word.strip() for word in topics_split[i]]\n",
    "        for q in x:\n",
    "            q_split = q.split()\n",
    "            check = any(item in q_split for item in word_list)\n",
    "            if check:\n",
    "                count += 1\n",
    "                pol += TextBlob(q).sentiment.polarity\n",
    "        if count != 0:\n",
    "            avg_pol = pol / count\n",
    "            topic_dict['Topic' + str(i+1)] = avg_pol\n",
    "        else:\n",
    "            topic_dict['Topic' + str(i+1)] = 0\n",
    "    return topic_dict\n",
    "\n",
    "\n",
    "topic_dict_question = getsentiment_topics(questions)\n",
    "topic_dict_answers = getsentiment_topics(answers)\n",
    "\n",
    "def merge_dict(x, y):\n",
    "    keys = x.keys()\n",
    "    values = zip(x.values(), y.values())\n",
    "    combined = dict(zip(keys, values))\n",
    "    avgdict = {}\n",
    "    for keys, values in combined.items():\n",
    "        avgdict[keys] = (values[0] + values[1]) / 2\n",
    "\n",
    "    return avgdict\n",
    "\n",
    "avg_dict = merge_dict(topic_dict_question, topic_dict_answers)\n",
    "print(avg_dict)\n",
    "\n",
    "keys = avg_dict.keys()\n",
    "values = avg_dict.values()\n",
    "clrs = ['red' if (x == max(values)) else 'blue' if (x == min(values)) else 'gray' for x in values]\n",
    "plt.bar(keys, values, color=clrs)\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Polarity')\n",
    "plt.title('Sentiment Analysis for each Topic')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(q_pos)\n",
    "\n",
    "\n",
    "# import operator\n",
    "# qmax = max(a_pos.items(), key=operator.itemgetter(1))[0]\n",
    "# qmin = min(a_neg.items(), key=operator.itemgetter(1))[0]\n",
    "#\n",
    "# print(qmax, qmin)\n",
    "\n",
    "#\n",
    "# print(len(q_pos), len(q_neu), len(q_neg))\n",
    "# print(len(a_pos), len(a_neu), len(a_neg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
